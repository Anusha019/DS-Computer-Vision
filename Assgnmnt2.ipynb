{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TMT53VWx-pm9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import helper\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from PIL import Image,ImageDraw,ImageFont\n",
        "from torch.optim import lr_scheduler\n",
        "# import splitfolders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CVyaagdt-3Fa"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh5b6Q9gLiyF"
      },
      "outputs": [],
      "source": [
        "# input_folder = 'weather_dataset/'\n",
        "\n",
        "# splitfolders.ratio(input_folder,\n",
        "#                    output=\"output\",\n",
        "#                    seed=42,\n",
        "#                    ratio=(.6, .2, .2),\n",
        "#                    group_prefix=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugA7Yqw1oJgi",
        "outputId": "bce75d7e-a62e-446e-f478-6486889a5b92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'val', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "os.listdir('output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lfBnMWb-50J",
        "outputId": "b76a9bf2-9a4c-4faa-eef6-acb86d20523d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class names 524\n",
            "524 batches in training\n",
            "175 batches in validation\n",
            "176 batches in test\n",
            "4187 training images\n",
            "1394 validation images\n",
            "1404 test images\n"
          ]
        }
      ],
      "source": [
        "data_transforms={\"train\":transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "                    ]),\n",
        "'val':transforms.Compose([transforms.Resize(256),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "                    ]),\n",
        "'test':transforms.Compose([transforms.Resize(256),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "                    ])}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join('output', x),data_transforms[x])for x in ['train', 'val','test']}\n",
        "dataloader={x:torch.utils.data.DataLoader(image_datasets[x],batch_size=8,shuffle=True,num_workers=2) for x in ['train', 'val','test'] }\n",
        "\n",
        "dataset_size={x :len(image_datasets[x]) for x in ['train', 'val','test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "print('class names {}'.format(len(dataloader['train'])))\n",
        "print('{} batches in training'.format(len(dataloader['train'])))\n",
        "print('{} batches in validation'.format(len(dataloader['val'])))\n",
        "print('{} batches in test'.format(len(dataloader['test'])))\n",
        "print('{} training images'.format(dataset_size['train']))\n",
        "print('{} validation images'.format(dataset_size['val']))\n",
        "print('{} test images'.format(dataset_size['test']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zQRxI7JWp5c",
        "outputId": "3d0e3cbc-4664-4329-f796-54aa907df2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n",
            "100%|██████████| 30.1M/30.1M [00:00<00:00, 68.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# model=torchvision.models.resnet34(pretrained=True)\n",
        "# model1=torchvision.models.efficientnet_v2_l(pretrained=True)\n",
        "model=torchvision.models.efficientnet_b1(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4jEiHNAzWd_k"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "num_ftrs=model.classifier[-1].in_features\n",
        "model.classifier[-1]=nn.Linear(num_ftrs,11)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.003,momentum=0.9)\n",
        "exp_lr_scheduler=lr_scheduler.StepLR(optimizer,step_size=7,gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWok2MWQaDDe",
        "outputId": "3df86f70-5676-4f9e-8f52-4cbda9584051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming training from epoch 45\n",
            "Epoch 46/50 - Train Loss: 0.6857 Acc: 0.7812  Val Loss: 0.6857 Acc: 0.7812\n",
            "Epoch 47/50 - Train Loss: 0.6814 Acc: 0.7740  Val Loss: 0.6814 Acc: 0.7740\n",
            "Epoch 48/50 - Train Loss: 0.6565 Acc: 0.7683  Val Loss: 0.6565 Acc: 0.7683\n",
            "Epoch 49/50 - Train Loss: 0.7178 Acc: 0.7654  Val Loss: 0.7178 Acc: 0.7654\n",
            "Epoch 50/50 - Train Loss: 0.6712 Acc: 0.7805  Val Loss: 0.6712 Acc: 0.7805\n"
          ]
        }
      ],
      "source": [
        "result=[]\n",
        "num_epochs = 50\n",
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "\n",
        "checkpoint_file = \"model_checkpoint1.pth\"\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    checkpoint = torch.load(checkpoint_file)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch,num_epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "  for inputs, labels in dataloader['train']:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  epoch_loss = running_loss / dataset_size['train']\n",
        "  epoch_acc = running_corrects.double() / dataset_size['train']\n",
        "  train_losses.append(epoch_loss)\n",
        "  train_accuracies.append(epoch_acc)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in dataloader['val']:\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  epoch_loss = running_loss / dataset_size['val']\n",
        "  epoch_acc = running_corrects.double() / dataset_size['val']\n",
        "  val_losses.append(epoch_loss)\n",
        "  val_accuracies.append(epoch_acc)\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}  Val Loss: {val_losses[-1]:.4f} Acc: {val_accuracies[-1]:.4f}')\n",
        "\n",
        "  checkpoint = {\n",
        "    \"epoch\": epoch + 1,  # Save the next epoch number\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),}\n",
        "  torch.save(checkpoint, \"model_checkpoint1.pth\")\n",
        "\n",
        "result.append({\n",
        "    'model': 'vgg16',\n",
        "    'train_loss': train_losses,\n",
        "    'train_accuracy': train_accuracies,\n",
        "    'val_loss': val_losses,\n",
        "    'val_accuracy': val_accuracies,\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHlY4UySJ-aC",
        "outputId": "e55a8079-bfbe-47c5-c82b-660f6aa18562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Accuracy 81.41025641025641\n",
            "Validation loss 0.5995633969364682\n",
            "1143 1404\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  correct=0\n",
        "  total=0\n",
        "  total_loss=0\n",
        "  for images,labels in dataloader['test']:\n",
        "    output=model(images)\n",
        "    _,predicted=torch.max(output.data,1)\n",
        "    total+=labels.size(0)\n",
        "    correct+=(predicted==labels).sum().item()\n",
        "\n",
        "    loss=criterion(output,labels)\n",
        "    total_loss+=loss.item()*labels.size(0)\n",
        "    avg_loss=total_loss/total\n",
        "\n",
        "  print('test Accuracy {}' .format(100 * correct/ total))\n",
        "  print('Validation loss {}'.format(avg_loss))\n",
        "  print(correct,total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HidiEhDzXnGh"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,num_epochs+1),val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxbjwXPeCoN3"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,num_epochs+1),train_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HaC-3iDEHmX"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,num_epochs+1),train_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo7Q4hqtELnS"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,num_epochs+1),val_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2jLKK55Ev5i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}